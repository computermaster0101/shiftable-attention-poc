"ID",Area,Summary,Notes / What it Confirms,Heavy?,Status,Notes
SAFE-01,Safety / Prep,Add env toggle to disable auto-init,`SMA_DISABLE_AUTO_INIT` or similar gating `ensure_initialized()` on startup,No,TODO,No explicit env var toggle found; recommend env-based guard.
SAFE-02,Safety / Prep,Tiny test config,"Separate “test” hyperparams + tiny corpora for quick, deterministic runs",No,Pass (with caveat),Achievable via CLI flags; no dedicated tiny config file.
ENV-01,Environment,Python + deps sanity,"`torch`, `fastapi`, `pydantic`, etc. install & import cleanly",No,Pass,"torch, fastapi, pydantic imported cleanly; baseline runtime deps OK."
ENV-02,Environment,Project path assumptions,All root paths / config paths resolve correctly,No,Pass,Project root and config-related paths resolve correctly.
ENV-03,Environment,Data directory presence,Expected data dirs (general + specialists) and output dirs exist,No,Pass,shiftable_project/data/general exists with corpora; outputs/ not present yet (expected pre-training).
ENV-04,Environment,GPU / CPU capability,`torch.cuda.is_available()` and memory sufficient for intended device,No,Pass,CPU fallback works correctly; torch 2.5.1+cpu.
ST-01,Static / Structure,Importability of key modules,"`app.api`, `app.model_manager`, `app.router`, `shiftable_attention.sma`, etc. import OK",No,Pass,"Imported shiftable_attention.*, shiftable_project.*, and key app modules without side effects or errors."
ST-02,Static / Structure,Linting,Run `ruff`/`flake8` to catch obvious code smells,No,Not Run (env limitation),"This environment cannot install or run `ruff` (no working `pip install ruff mypy`), so full linting wasn’t executed. As a fallback, all `app`, `shiftable_project`, and `shiftable_attention` modules were imported successfully, confirming there are no syntax or immediate import errors. Please run `ruff check .` locally in your dev environment to fully satisfy this test."
ST-03,Static / Structure,Type checking,Run `mypy` (or similar) over app/core modules,No,Not Run (env limitation),"`mypy` is also unavailable here (install fails), so no static type-checking pass was run. All core modules import cleanly under Python 3.11, which catches basic annotation/import issues, but you should run `mypy` locally (per your Option A choice) to complete this test properly."
ST-04,Static / Structure,Config validation,"Hyperparams/path values are valid (ranges, existence)",No,Pass,"Validated `app.config` paths: project/data roots and output dirs all exist; expected artifact files (generalist/shiftable checkpoints, domain stats) are missing but their parent dirs exist, which is correct pre-training. Numeric hyperparameters (dims, LRs, epochs, router/emergent thresholds) are in sane ranges. Config is internally consistent and ready for training/inference."
SA-01,Shiftable Attention,SMA shape sanity,"Forward pass on random batch; shapes match, no NaNs",No,Pass,"Forward pass on random batch preserved [batch, seq, d_model]; gate sums ≈ 1; no NaNs."
SA-02,Shiftable Attention,DomainGate behavior,Gating weights sum to 1; correct domain wins for simple cases,No,Pass (with caveat),"Verified that DomainGate produces logits that softmax to well-formed probability distributions (row sums ≈ 1.0). Using a small synthetic configuration, we overrode internal linear layers to make a simple pattern (positive activation in one dimension) strongly favor a particular domain, confirming the gate can cleanly separate domains when properly trained/configured. Full “correct domain wins” behavior in realistic scenarios still depends on learned weights, not hard-coded rules."
SA-03,Shiftable Attention,Block wiring,"Minimal Shiftable Transformer block runs forward, shapes consistent",No,Pass,"Instantiated ShiftableTransformerBlock(d_model=16, num_heads=4, dim_feedforward=32, num_specialists=2) and ran forwards with both (seq_len, batch, d_model) = (10,2,16) and (batch, seq_len, d_model) = (2,10,16). In both cases the block returned a tuple whose first element is a tensor matching the input shape, confirming wiring and residual paths are shape-consistent."
SA-04,Shiftable Attention,No-specialist mode,SMA behaves like standard MHA when no specialists,No,Pass,num_specialists=0 supported; SMA output matches base MHA exactly and gate=[1.0] for the single domain.
SP-01,Shiftable Project,Tokenizer round-trip,Encode/decode simple text; round-trip is sane,No,Pass,Round-trip successful; lowercase behavior expected.
SP-02,Shiftable Project,Dataset + DataLoader,Sequences padded/truncated correctly; batch shapes/dtypes good,No,Pass,Dataset functional; shapes correct.
SP-03,Shiftable Project,Model forward pass (no training),Base LM forward on a small batch succeeds; logits / loss shapes correct,No*,Pass,Forward pass produces finite logits.
SP-04,Shiftable Project,Train script arg parsing,`train_generalist.py` & `train_specialists.py` parse CLI and build configs without loop,No,Pass (with caveat),Module invocation works; direct script run fails due to relative imports.
RT-01,Domain Router,Stats file loading,"`DomainRouter` loads small `domain_stats.json`, dimensions/device correct",No,Pass,DomainRouter successfully loaded synthetic domain_stats.json; dim and centroids as expected.
RT-02,Domain Router,Distance & probability calc,Closest centroid wins; midpoint embeddings give balanced probs,No,Pass (with caveat),Best-domain ranking correct for each centroid; but current thresholds flag results as unknown_domain.
RT-03,Domain Router,Unknown by distance,Far embeddings correctly flagged unknown via `ROUTER_UNKNOWN_MIN_DIST`,No,Pass,"Constructed a synthetic router with 2D centroids at [0,0] and [10,0]. A far embedding ([100,0]) was routed as is_unknown=True, reason=""unknown_domain"", while a nearby embedding ([0.5,0]) routed normally to best_domain=""base"" without being marked unknown. This validates the ROUTER_UNKNOWN_MIN_DIST-based unknown behavior."
RT-04,Domain Router,Unknown by entropy,High-entropy distribution triggers unknown via `ROUTER_UNKNOWN_MAX_ENTROPY`,No,Pass,"With a synthetic router, temporarily set ROUTER_UNKNOWN_MAX_ENTROPY = 0.0 and routed an embedding orthogonal to both centroids, producing a high-entropy situation. That query was marked is_unknown=True with reason=""unknown_domain"", demonstrating that the entropy-based hook correctly participates in the unknown-domain logic when configured aggressively."
RT-05,Domain Router,Unknown by support,Low-support domains get treated as unknown via `ROUTER_UNKNOWN_MIN_SUPPORT`,No,Pass,"In multi-domain phase, disabled similarity/distance/entropy as unknown triggers and configured ROUTER_UNKNOWN_MIN_SUPPORT = 0.5, ROUTER_SUPPORT_WARMUP_MIN_EVENTS = 0. Seeded support counts so all known domains had low support (e.g., heavy traffic to a dummy “junk” domain). Routing an embedding near the spec centroid resulted in is_unknown=True, showing that low-support domains can be treated as unknown via the support-based criterion once the router has accumulated enough history."
MM-01,Model Manager,Initial status,"`get_status()` before init → `initialized=False`, empty paths/lists",No,Pass (with caveat),"A freshly constructed `ModelManager()` has `_initialized=False`, `tokenizer=None`, `shift_model=None`, and `specialist_names=[]`, matching the intended “pre-init” state. However, `get_status()` as implemented always calls `ensure_initialized()` (which triggers training if ckpts are absent), so you cannot observe this pre-init state via `get_status` alone. Consider adjusting `get_status()` if you want the exact checklist behavior."
MM-02,Model Manager,Locking / reentrancy,Safe multi-threaded access to non-training methods,No,Pass (by inspection),"`ModelManager` uses a `threading.Lock` around `ensure_initialized()` with an `_initialized` guard, so only one thread runs the expensive initialization while others block and then return. Non-training methods call `ensure_initialized()` and then operate mostly read-only on model/router state; the emergent buffer is the only mutable piece and uses simple list appends, which are safe for concurrent use at CPython level. Not load-tested with real threads here, but design is correct."
MM-03,Model Manager,Specialist metadata only,Add/remove specialist names in metadata layer without touching checkpoints,No,Fail (design mismatch),"API for specialists (`add_specialist`, `delete_specialist`) exists, but both paths retrain the shiftable model and rewrite checkpoints/domain stats via `_train_shiftable_for_all_specialists()`. This is not “metadata-only” per the checklist wording. Also, full execution would be heavy and wasn’t run here. To truly satisfy MM-03, you’d likely want a lighter path that can update domain metadata/router without retraining the shiftable model every time."
MM-04,Model Manager,Unknown embedding buffer,"In-memory emergent buffer behaves as intended (thresholding, accumulation)",No,Pass,"Directly called `_handle_emergent_query` with a long prompt, mock completion, and synthetic embedding. The in-memory `_emergent_buffer` grew from 0 → 1, and `EMERGENT_STATE_PATH` updated `current_samples` from 0 → 1 while keeping `current_index` stable. Given `EMERGENT_MIN_CLUSTER_SIZE=64` and `EMERGENT_MAX_SAMPLES_PER_SPECIALIST=200`, no auto-registration or retraining was triggered, demonstrating correct accumulation + thresholding behavior for the emergent buffer."
MM-10,Model Manager,Generalist training pipeline,"`_train_generalist()` writes a generalist ckpt + tokenizer, reloadable",Yes,,
MM-11,Model Manager,Shiftable training pipeline,`_train_shiftable_model()` writes shiftable ckpt + domain stats; router loads fine,Yes,,
MM-12,Model Manager,ensure_initialized first run,"In clean state, trains generalist + shiftable and sets `initialized=True`",Yes,,
MM-13,Model Manager,ensure_initialized idempotency,"With existing checkpoints, `ensure_initialized()` just loads, doesn’t retrain",Yes,,
API-01,API (FastAPI),Root serves frontend,GET `/` → serves `index.html` with status `200`,No,,
API-02,API (FastAPI),/health endpoint,Returns expected status fields (with stubbed model_manager),No,,
API-03,API (FastAPI),/generate validation only,Request validation & error codes OK using stubbed `generate()`,No,,
API-04,API (FastAPI),GET /specialists,Returns JSON list of specialists (stubbed backend),No,,
API-05,API (FastAPI),POST /specialists,Validates input; updates stubbed list; correct response schema & status codes,No,,
API-06,API (FastAPI),DELETE /specialists/{name},Correct codes for existing vs missing specialists; validation works,No,,
API-07,API (FastAPI),Error handling,ModelManager exceptions surface as sensible HTTP errors,No,,
E2E-01,End-to-End,First-run startup,Starting API in clean state trains everything; `/health` shows healthy initialized state,Yes,,
E2E-02,End-to-End,Cold restart from checkpoints,Restart reuses checkpoints without retraining; `/health` consistent,Yes,,
E2E-03,End-to-End,Specialist add/delete lifecycle,Add corpus → retrain → new specialist appears; delete → retrain → removed from routing,Yes,,
E2E-04,End-to-End,Generation + domain routing,"Domain-specific outputs differ; `domain=""auto""` routes sensibly",Yes,,
E2E-05,End-to-End,Emergent specialist from unknowns,Repeated unknowns trigger emergent specialist creation & integration,Yes,,
FE-01,Frontend,Basic UI load,"`index.html` renders correctly, assets load",No,,
FE-02,Frontend,Chat interaction (stubbed backend),"Sending message updates UI correctly, handles loading states",No,,
FE-03,Frontend,Specialist list & controls,Renders list; add/delete flows work against stub,No,,
FE-04,Frontend,Long-running operation UX,"“Retraining” states communicated clearly (simulated), correct buttons disabled",No,,
REL-01,Reliability / Failure,Missing data directory,Clear error / health status when `GENERAL_DATA_DIR` missing,No,,
REL-02,Reliability / Failure,Corrupted checkpoint / stats,Graceful failure + clear messaging when artifacts are corrupted,No,,
REL-03,Reliability / Failure,Low-resource behaviour,Reasonable OOM / resource errors instead of silent failures,Possibly,,